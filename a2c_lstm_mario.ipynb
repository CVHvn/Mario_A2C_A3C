{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19939a9c-03f0-4150-9477-9109e9735319",
      "metadata": {
        "id": "19939a9c-03f0-4150-9477-9109e9735319"
      },
      "source": [
        "# install packages\n",
        "install packages need to train mario agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a21d2d-dccd-447b-84e8-37c55db5ef2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44a21d2d-dccd-447b-84e8-37c55db5ef2d",
        "outputId": "2a5da188-df12-4bf5-a301-5804fc9d068d"
      },
      "outputs": [],
      "source": [
        "# !pip install torch\n",
        "# !pip install matplotlib\n",
        "!pip install gymnasium==0.29.1\n",
        "!pip install gym-super-mario-bros==7.4.0\n",
        "!pip install gym==0.25.2\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install imageio\n",
        "# !pip install torchvision\n",
        "!pip install opencv-python-headless\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829519d4-5bf4-46ed-8b52-8f03fee2f8d9",
      "metadata": {
        "id": "829519d4-5bf4-46ed-8b52-8f03fee2f8d9"
      },
      "source": [
        "# import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7144ed99-8f5f-489c-b6b4-2ce51f9b9d8f",
      "metadata": {
        "id": "7144ed99-8f5f-489c-b6b4-2ce51f9b9d8f"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import copy\n",
        "import cv2\n",
        "import imageio\n",
        "import numpy as np\n",
        "import random, os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "#import multiprocessing as mp\n",
        "from torchvision import transforms as T\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd03a2e1-8ccd-4dd5-a13d-87f265c1c55e",
      "metadata": {
        "id": "bd03a2e1-8ccd-4dd5-a13d-87f265c1c55e"
      },
      "source": [
        "# Create hyperparammeters\n",
        "Config hyperparammeters, just change it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a95999c-5111-4054-9256-45cd206a697e",
      "metadata": {
        "id": "4a95999c-5111-4054-9256-45cd206a697e"
      },
      "outputs": [],
      "source": [
        "#class DictWrapper created by Chatgpt\n",
        "class DictWrapper:\n",
        "    def __init__(self, dictionary):\n",
        "        self._dict = dictionary\n",
        "\n",
        "    def __getattr__(self, item):\n",
        "        if item in self._dict:\n",
        "            return self._dict[item]\n",
        "        else:\n",
        "            raise AttributeError(f\"'DictWrapper' object has no attribute '{item}'\")\n",
        "\n",
        "config = {\n",
        "    'num_envs': 16,\n",
        "    'save_model_step': int(1e5),\n",
        "    'save_figure_step': int(1e3),\n",
        "    'learn_step': 20,\n",
        "    'total_step_or_episode': 'step',\n",
        "    'total_step': int(5e6),\n",
        "    'total_episode': None,\n",
        "    'save_dir': \"\",\n",
        "    'gamma': 0.99,\n",
        "    'learning_rate': 1e-4,\n",
        "    'state_dim': (1, 84, 84),\n",
        "    'action_dim': 12,#12 for complex, 7 for simple\n",
        "    'entropy_coef': 0.01,\n",
        "    'V_coef': 0.5,\n",
        "    'max_grad_norm': 0.5,\n",
        "    'world': 1,\n",
        "    'stage': 1,\n",
        "    'action_type': 'complex',\n",
        "    'optimizer_eps': 1e-8, #or 1e-5\n",
        "    'detach_lstm_state': False, #If True, Model just use h, c as inputs. If False, LSTM will backpropagation through time.\n",
        "    'init_weights': True #use _initialize_weights function or not \n",
        "}\n",
        "\n",
        "config = DictWrapper(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ha80HrVm1_OR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ha80HrVm1_OR",
        "outputId": "05c74b3e-1ae5-4b63-b38d-7fd2089a7ba2"
      },
      "outputs": [],
      "source": [
        "gym.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "440a3f9c-d1fd-41e5-a431-592e719aef84",
      "metadata": {
        "id": "440a3f9c-d1fd-41e5-a431-592e719aef84"
      },
      "source": [
        "# Define environment\n",
        "## Create a custom environment, We need:\n",
        "- SkipFrame: Because the episode is very long, we only need to repeat actions sometimes in this environment. We repeat each action 4 times and skip the first 3 frames (returning the 4th frame). We also sum the rewards from all 4 frames.\n",
        "- GrayScaleResizeObservation: Convert the state to grayscale (from RGB to a gray image) and resize it to 84x84 pixels.\n",
        "- NoopResetEnv: When resetting the environment, we perform random actions before starting the environment. This is similar to the Atari strategy. When resetting, we randomly choose num_noops actions between 0 and noop_max and perform num_noops random actions. If the random actions lead to a terminal state, we reset and continue performing random actions. I set noop_max to 30, similar to Atari.\n",
        "- CustomRewardAndDoneEnv\n",
        "    - I noticed that many people train Mario using this custom reward system, so I copied it. The system adds 50 reward points if the agent solves the stage and subtracts 50 reward points if the agent dies. The reward is divided by 10. I set done to True if Mario dies, instead of the default setting where Mario loses all lives.\n",
        "    - Stage 4-2: Subtract 50 reward points if Mario moves on top of the map (y_pos >= 255).\n",
        "    - Stages 4-4 and 7-4: Set done = True when Mario goes the wrong way and subtract 50 reward points as a penalty. If Mario takes the correct path but the map still loops (a known bug), I set done = True but do not apply a penalty.\n",
        "    - Stages 4-4: give a -0.1 reward every step.\n",
        "\n",
        "\n",
        "## About reward system:\n",
        "- Set done to True when Mario dies: This is the most important aspect because, in the default reward system, Mario still gains a reward by just moving right. If Mario dies, the agent doesn't lose total rewards and can continue moving right (in the new life) to get more rewards. This is the easiest way for the agent to earn rewards, and it can learn to exploit this trick.\n",
        "- Penalty of -50 reward when Mario dies: This is necessary to speed up Mario's training. Without this penalty, the agent may struggle to complete more difficult stages.\n",
        "- Reward of 50 when reaching the flag: This encourages Mario to train faster and overcome difficult sections in harder stages.\n",
        "- Changing the penalty and flag reward to more or less than 50 doesn't make a significant difference, so I haven't changed it.\n",
        "- Divide rewards by 10: I believe this reduces the total rewards and helps the agent learn a better strategy, but I'm not entirely sure how necessary this is. I simply followed an existing approach.\n",
        "- With Stage 4-2: I noticed that the agent can earn more rewards when Mario goes to the warp zone, but Mario can't win this stage using the warp zone because the reward system gives negative rewards when Mario moves left. Therefore, I added a penalty when Mario moves to the top of the map.\n",
        "- Use FrameStack to stack the latest 4 frames as observation.\n",
        "- With Stage 4-4 and 7-4:\n",
        "    - Since this map has a wrong path, Mario can enter a loop where the reward increases indefinitely. To prevent this, I set done = True and assign a negative penalty reward.\n",
        "    - I also give a negative reward to prevent Mario from taking the wrong path.\n",
        "    - Another strategy is to give a negative reward without setting done = True (as in 4-2). However, this strategy doesn't work due to a bug in this map.\n",
        "    - Even when Mario is on the correct path, sometimes he still enters the loop. To handle this, I set done = True every time Mario enters the loop (checked by x_pos and max_x_pos).\n",
        "- Stage 4-4: Assign a -0.1 reward for every step: This prevents Mario from getting stuck. This map has a section where Mario needs to move left, but moving left incurs a negative reward in the default system. If Mario moves right, he takes the wrong path, causing the episode to end with a negative reward. To keep Mario moving, I added a negative reward for every step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba268b64-6d59-4682-ac6e-f4118bd2d6da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba268b64-6d59-4682-ac6e-f4118bd2d6da",
        "outputId": "1199a203-1dad-48a7-b1f9-e8c9a61d9cc0"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, trunk, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, trunk, info\n",
        "\n",
        "class GrayScaleResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.current_state = observation\n",
        "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
        "        observation = observation.astype(np.uint8).reshape(-1, observation.shape[0], observation.shape[1])\n",
        "        return observation\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        super(NoopResetEnv, self).__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        obs = self.env.reset(**kwargs)\n",
        "        noops = np.random.randint(0, self.noop_max, (1, ))[0]\n",
        "        for _ in range(noops):\n",
        "            action = self.env.action_space.sample()\n",
        "            obs, _, done, _, _ = self.env.step(action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        obs, reward, done, trunk, info = self.env.step(ac)\n",
        "        return obs, reward, done, trunk, info\n",
        "\n",
        "class CustomRewardAndDoneEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, world=1, stage=1, additional_bonus_state_8_4_option = \"no\"):\n",
        "        super(CustomRewardAndDoneEnv, self).__init__(env)\n",
        "        self.current_score = 0\n",
        "        self.current_x = 0\n",
        "        self.old_x = -1\n",
        "        self.current_x_count = 0\n",
        "        self.max_x = 0\n",
        "        self.world = world\n",
        "        self.stage = stage\n",
        "        if self.world == 8 and self.stage == 4:\n",
        "            self.sea_map = False\n",
        "            self.additional_bonus_state_8_4_option = additional_bonus_state_8_4_option\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.current_score = 0\n",
        "        self.current_x = 0\n",
        "        self.old_x = -1\n",
        "        self.current_x_count = 0\n",
        "        self.max_x = 0\n",
        "        if self.world == 8 and self.stage == 4:\n",
        "            self.sea_map = False\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done, trunc, info = self.env.step(action)\n",
        "\n",
        "        if (info['x_pos'] - self.current_x) == 0:\n",
        "            self.current_x_count += 1\n",
        "        else:\n",
        "            self.current_x_count = 0\n",
        "        if info[\"flag_get\"]:\n",
        "            reward += 50\n",
        "            done = True\n",
        "        if done and info[\"flag_get\"] == False and info[\"time\"] != 0:\n",
        "            reward -= 50\n",
        "            done = True\n",
        "        self.current_x = info[\"x_pos\"]\n",
        "\n",
        "        if self.world == 7 and self.stage == 4:\n",
        "            if (506 <= info[\"x_pos\"] <= 832 and info[\"y_pos\"] > 127) or (\n",
        "                    832 < info[\"x_pos\"] <= 1064 and info[\"y_pos\"] < 80) or (\n",
        "                    1113 < info[\"x_pos\"] <= 1464 and info[\"y_pos\"] < 191) or (\n",
        "                    1579 < info[\"x_pos\"] <= 1943 and info[\"y_pos\"] < 191) or (\n",
        "                    1946 < info[\"x_pos\"] <= 1964 and info[\"y_pos\"] >= 191) or (\n",
        "                    1984 < info[\"x_pos\"] <= 2060 and (info[\"y_pos\"] >= 191 or info[\"y_pos\"] < 127)) or (\n",
        "                    2114 < info[\"x_pos\"] < 2440 and info[\"y_pos\"] < 191):\n",
        "                reward -= 50\n",
        "                done = True\n",
        "            if done == False and info[\"x_pos\"] < self.max_x - 100:\n",
        "                done = True\n",
        "        if self.world == 4 and self.stage == 4:\n",
        "            if (info[\"x_pos\"] <= 1500 and info[\"y_pos\"] < 127) or (\n",
        "                    1588 <= info[\"x_pos\"] < 2380 and info[\"y_pos\"] >= 127):\n",
        "                reward -= 50\n",
        "                done = True\n",
        "            if done == False and info[\"x_pos\"] < self.max_x - 100:\n",
        "                done = True\n",
        "            if done == False:\n",
        "                reward -= 0.1\n",
        "        if self.world == 4 and self.stage == 2 and done == False and info['y_pos'] >= 255:\n",
        "            reward -= 50\n",
        "        if self.world == 8 and self.stage == 4:\n",
        "            if info[\"x_pos\"] > 2440 and info[\"x_pos\"] <= 2500:\n",
        "                done = True\n",
        "                reward -= 100\n",
        "            if info[\"x_pos\"] >= 3675 and info[\"x_pos\"] <= 3700:\n",
        "                done = True\n",
        "                reward -= 50\n",
        "\n",
        "            if info[\"x_pos\"] < self.max_x - 200:\n",
        "                if self.max_x >= 1240 and self.max_x <= 1310: #solved bug because x_pos duplicated\n",
        "                    if info[\"x_pos\"] >= 320:\n",
        "                        done = True\n",
        "                        reward -= 50\n",
        "\n",
        "            if info[\"x_pos\"] < self.old_x - 200:\n",
        "                if info[\"x_pos\"] >= 312-5 and info[\"x_pos\"] <= 312+5:\n",
        "                    done = True\n",
        "                    reward -= 50\n",
        "                elif info[\"x_pos\"] >= 56-5 and info[\"x_pos\"] <= 56+5 and self.max_x > 3645 and self.sea_map == False:\n",
        "                    if self.additional_bonus_state_8_4_option == 'right_pipe':\n",
        "                        reward += 50\n",
        "                    self.sea_map = True\n",
        "            if self.additional_bonus_state_8_4_option == 'right_pipe':\n",
        "                if info[\"x_pos\"] > self.max_x + 100:\n",
        "                    reward += 50\n",
        "            if done == False:\n",
        "                reward -= 0.1\n",
        "        self.max_x = max(self.max_x, self.current_x)\n",
        "        self.current_score = info[\"score\"]\n",
        "        self.old_x = self.current_x\n",
        "\n",
        "        return state, reward / 10., done, trunc, info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec30b8e-1b8d-487f-98f3-21615ce704ee",
      "metadata": {
        "id": "6ec30b8e-1b8d-487f-98f3-21615ce704ee"
      },
      "source": [
        "# Create MultipleEnvironments\n",
        "MultipleEnvironments use multi-processing to parallel running.\n",
        "\n",
        "Because in the training process, we need to reset the environment when the agent reaches the terminal state. But if we will do it in parallel, then I don't want to check each environment and reset (by loop) or create a new function that parallels check and reset all environments. Then I reset the environment if done = True in step function and set next_state = env.reset(). Then in training, we just set state = next_state (next_state is reset state if done = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y2mcbU_JvQwI",
      "metadata": {
        "id": "y2mcbU_JvQwI"
      },
      "outputs": [],
      "source": [
        "#modify from https://github.com/uvipen/Super-mario-bros-PPO-pytorch/blob/master/src/env.py\n",
        "def create_env(world, stage, action_type, test=False):\n",
        "    if gym.__version__ < '0.26':\n",
        "        env = gym_super_mario_bros.make(f\"SuperMarioBros-{world}-{stage}-v0\", new_step_api=True)\n",
        "    else:\n",
        "        env = gym_super_mario_bros.make(f\"SuperMarioBros-{world}-{stage}-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "    if action_type == \"right\":\n",
        "        action_type = RIGHT_ONLY\n",
        "    elif action_type == \"simple\":\n",
        "        action_type = SIMPLE_MOVEMENT\n",
        "    else:\n",
        "        action_type = COMPLEX_MOVEMENT\n",
        "\n",
        "    env = JoypadSpace(env, action_type)\n",
        "\n",
        "    if test == False:\n",
        "        env = NoopResetEnv(env)\n",
        "    env = SkipFrame(env, skip=4)\n",
        "    env = CustomRewardAndDoneEnv(env, world, stage)\n",
        "    env = GrayScaleResizeObservation(env, shape=84)\n",
        "    # if gym.__version__ < '0.26':\n",
        "    #     env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "    # else:\n",
        "    #     env = FrameStack(env, num_stack=4)\n",
        "    return env\n",
        "\n",
        "class MultipleEnvironments:\n",
        "    def __init__(self, world, stage, action_type, num_envs):\n",
        "        self.agent_conns, self.env_conns = zip(*[mp.Pipe(duplex=True) for _ in range(num_envs)])\n",
        "        self.envs = [create_env(world, stage, action_type) for _ in range(num_envs)]\n",
        "\n",
        "        for index in range(num_envs):\n",
        "            process = mp.Process(target=self.run, args=(index,))\n",
        "            process.start()\n",
        "            self.env_conns[index].close()\n",
        "\n",
        "    def run(self, index):\n",
        "        self.agent_conns[index].close()\n",
        "        while True:\n",
        "            request, action = self.env_conns[index].recv()\n",
        "            if request == \"step\":\n",
        "                next_state, reward, done, trunc, info = self.envs[index].step(action)\n",
        "                if done:\n",
        "                    next_state = self.envs[index].reset()\n",
        "                self.env_conns[index].send((next_state, reward, done, trunc, info))\n",
        "            elif request == \"reset\":\n",
        "                self.env_conns[index].send(self.envs[index].reset())\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "    def step(self, actions):\n",
        "        [agent_conn.send((\"step\", act)) for agent_conn, act in zip(self.agent_conns, actions)]\n",
        "        next_states, rewards, dones, truncs, infos = zip(*[agent_conn.recv() for agent_conn in self.agent_conns])\n",
        "        return next_states, rewards, dones, truncs, infos\n",
        "\n",
        "    def reset(self):\n",
        "        [agent_conn.send((\"reset\", None)) for agent_conn in self.agent_conns]\n",
        "        states = [agent_conn.recv() for agent_conn in self.agent_conns]\n",
        "        return states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31dycPWmOuOR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31dycPWmOuOR",
        "outputId": "ebfb5c73-f02d-4bf3-f2f3-026ee179b9b7"
      },
      "outputs": [],
      "source": [
        "mp.cpu_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a894f8-f303-4fac-b478-0b191f38f274",
      "metadata": {
        "id": "92a894f8-f303-4fac-b478-0b191f38f274"
      },
      "source": [
        "# Create memory\n",
        "Memory just save all info we need to train and return all stored info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1736a4f8-d863-4bed-b290-3812f6c43eae",
      "metadata": {
        "id": "1736a4f8-d863-4bed-b290-3812f6c43eae"
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "    def __init__(self, num_envs):\n",
        "        self.num_envs = num_envs\n",
        "\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.next_states = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.logits = []\n",
        "        self.values = []\n",
        "\n",
        "    def save(self, state, action, reward, next_state, done, logit, V):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.next_states.append(next_state)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.logits.append(logit)\n",
        "        self.values.append(V)\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.next_states = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.logits = []\n",
        "        self.values = []\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.states, self.actions, self.next_states, self.rewards, self.dones, self.logits, self.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4303a12-57dc-46c5-9134-5d2e20e0438f",
      "metadata": {
        "id": "c4303a12-57dc-46c5-9134-5d2e20e0438f"
      },
      "source": [
        "# Create agent\n",
        "The agent includes 4 main functions:\n",
        "## train\n",
        "train function train agent via many episodes:\n",
        "- first, we create (h, c) as zeros. Reset the first state.\n",
        "- loop until the agent wins this stage or reaches the maximum episode/step:\n",
        "    - predict value, logit, h, c for the current state\n",
        "    - sample action from logit with category distribution (select_action function)\n",
        "    - log all info to memory\n",
        "- train agent every learn_step (learn function), detach h,c after learn_step (because after backward, we need clear gradient)\n",
        "- eval agent every save_figure_step (save_figure function)\n",
        "- reset (h, c) to zeros if the agent reaches the terminal state\n",
        "- set state = next_state (I reset environment when agent reach terminal state then next_state is first state if done=True)\n",
        "\n",
        "## select_action\n",
        "this function sample action from logit:\n",
        "- just convert logit to probability: policy = F.softmax(logits, dim=1)\n",
        "- create distribution from probability: distribution = torch.distributions.Categorical(policy)\n",
        "- sample action from distribution: actions = distribution.sample()\n",
        "\n",
        "## save_figure\n",
        "this function eval agent and saves agent/video if the agent yields better total rewards:\n",
        "- create (h, c) as zeros, and reset the environment.\n",
        "- loop until the agent reaches the terminal state.\n",
        "    - predict logit from model\n",
        "    - get action = argmax (logit)\n",
        "    - environment do this action to get next_state, reward, info, done\n",
        "- if total_reward > best test total reward or agent complete this stage, I save model and video.\n",
        "- if agent completes this state, we stop training.\n",
        "\n",
        "## learn\n",
        "this function trains agent from the experiment saved in memory\n",
        "- get all the info from memory\n",
        "- calculate td (lambda) target and gae advantages\n",
        "- calculate loss\n",
        "- norm gradient\n",
        "- update model from loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d-yL01WMumqF",
      "metadata": {
        "id": "d-yL01WMumqF"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, world, stage, action_type, envs, num_envs, state_dim, action_dim, save_dir, save_model_step,\n",
        "                 save_figure_step, learn_step, total_step_or_episode, total_step, total_episode, model,\n",
        "                 gamma, learning_rate, entropy_coef, V_coef, max_grad_norm, optimizer_eps, device):\n",
        "        self.world = world\n",
        "        self.stage = stage\n",
        "        self.action_type = action_type\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "        self.learn_step = learn_step\n",
        "        self.total_step_or_episode = total_step_or_episode\n",
        "        self.total_step = total_step\n",
        "        self.total_episode = total_episode\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.current_episode = 0\n",
        "\n",
        "        self.save_model_step = save_model_step\n",
        "        self.save_figure_step = save_figure_step\n",
        "\n",
        "        self.device = device\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.num_envs = num_envs\n",
        "        self.envs = envs\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.V_coef = V_coef\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, eps = optimizer_eps)\n",
        "\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "        self.memory = Memory(self.num_envs)\n",
        "\n",
        "        self.is_completed = False\n",
        "\n",
        "        self.env = None\n",
        "        self.max_test_score = -1e9\n",
        "\n",
        "        # I just log 1000 lastest update and print it to log.\n",
        "        self.V_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.P_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.E_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.total_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.loss_index = 0\n",
        "        self.len_loss = 0\n",
        "\n",
        "    def save_figure(self, is_training=False):\n",
        "        # test current model and save model/figure if model yield best total rewards.\n",
        "        # create env for testing, reset test env\n",
        "        if self.env is None:\n",
        "            self.env = create_env(self.world, self.stage, self.action_type, True)\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "\n",
        "        images = []\n",
        "        total_reward = 0\n",
        "        total_step = 0\n",
        "        num_repeat_action = 0\n",
        "        old_action = -1\n",
        "\n",
        "        # create h, c as zeros\n",
        "        h = torch.zeros((1, 512), dtype=torch.float, device = self.device)\n",
        "        c = torch.zeros((1, 512), dtype=torch.float, device = self.device)\n",
        "\n",
        "        episode_time = datetime.now()\n",
        "\n",
        "        # play 1 episode, just get loop action with max probability from model until the episode end.\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                logit, V, h, c = self.model(torch.tensor(state, dtype = torch.float, device = self.device).unsqueeze(0), h, c)\n",
        "            action = logit.argmax(-1).item()\n",
        "            next_state, reward, done, trunc, info = self.env.step(action)\n",
        "            state = next_state\n",
        "            img = Image.fromarray(self.env.current_state)\n",
        "            images.append(img)\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "\n",
        "            if action == old_action:\n",
        "                num_repeat_action += 1\n",
        "            else:\n",
        "                num_repeat_action = 0\n",
        "            old_action = action\n",
        "            if num_repeat_action == 200:\n",
        "                break\n",
        "\n",
        "        #logging, if model yield better result, save figure (test_episode.mp4) and model (best_model.pth)\n",
        "        if is_training:\n",
        "            f_out = open(f\"logging_test.txt\", \"a\")\n",
        "            f_out.write(f'episode_reward: {total_reward} episode_step: {total_step} current_step: {self.current_step} loss_p: {(self.P_loss.sum()/self.len_loss):.4f} loss_v: {(self.V_loss.sum()/self.len_loss):.4f} loss_e: {(self.E_loss.sum()/self.len_loss):.4f} loss: {(self.total_loss.sum()/self.len_loss):.4f} episode_time: {datetime.now() - episode_time}\\n')\n",
        "            f_out.close()\n",
        "\n",
        "        if total_reward > self.max_test_score or info['flag_get']:\n",
        "            imageio.mimsave('test_episode.mp4', images)\n",
        "            self.max_test_score = total_reward\n",
        "            if is_training:\n",
        "                torch.save(self.model.state_dict(), f\"best_model.pth\")\n",
        "\n",
        "        # if model can complete this game, stop training by set self.is_completed to True\n",
        "        if info['flag_get']:\n",
        "            self.is_completed = True\n",
        "\n",
        "    def save_model(self):\n",
        "        torch.save(self.model.state_dict(), f\"model_{self.current_step}.pth\")\n",
        "\n",
        "    def load_model(self, model_path = None):\n",
        "        if model_path is None:\n",
        "            model_path = f\"model_{self.current_step}.pth\"\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    def select_action(self, states, h, c):\n",
        "        # select action when training, we need use Categorical distribution to make action base on probability from model\n",
        "        states = torch.tensor(np.array(states), device = self.device)\n",
        "\n",
        "        logits, V, h, c = self.model(states, h, c)\n",
        "        with torch.no_grad():\n",
        "            policy = F.softmax(logits, dim=1)\n",
        "            distribution = torch.distributions.Categorical(policy)\n",
        "            actions = distribution.sample().cpu().numpy().tolist()\n",
        "        return actions, logits, V, h, c\n",
        "\n",
        "    def update_loss_statis(self, loss_p, loss_v, loss_e, loss):\n",
        "        # update loss for logging, just save 1000 latest updates.\n",
        "        self.V_loss[self.loss_index] = loss_v\n",
        "        self.P_loss[self.loss_index] = loss_p\n",
        "        self.E_loss[self.loss_index] = loss_e\n",
        "        self.total_loss[self.loss_index] = loss\n",
        "        self.loss_index = (self.loss_index + 1)%1000\n",
        "        self.len_loss = min(self.len_loss+1, 1000)\n",
        "\n",
        "    def learn(self, h, c):\n",
        "        # reset optimizer\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # get all data\n",
        "        states, actions, next_states, rewards, dones, logits, values = self.memory.get_data()\n",
        "\n",
        "        # calculate target (td lambda target) and gae advantages\n",
        "        targets = []\n",
        "        with torch.no_grad():\n",
        "            _, next_value, h, c = self.model(torch.tensor(np.array(next_states[-1]), device = self.device), h, c)\n",
        "        target = next_value\n",
        "        advantage = 0\n",
        "\n",
        "        for state, next_state, reward, done, V in zip(states[::-1], next_states[::-1], rewards[::-1], dones[::-1], values[::-1]):\n",
        "            done = torch.tensor(done, device = self.device, dtype = torch.float).reshape(-1, 1)\n",
        "            reward = torch.tensor(reward, device = self.device).reshape(-1, 1)\n",
        "\n",
        "            target = next_value * self.gamma * (1-done) + reward\n",
        "            advantage = target + self.gamma * advantage * (1-done)\n",
        "            targets.append(advantage)\n",
        "            advantage = advantage - V.detach()\n",
        "            next_value = V.detach()\n",
        "        targets = targets[::-1]\n",
        "\n",
        "        # convert all data to tensor\n",
        "        values = torch.cat(values, 0)\n",
        "        targets = torch.cat(targets, 0).view(-1, 1)\n",
        "        logits = torch.cat(logits, 0)\n",
        "        probs = torch.softmax(logits, -1)\n",
        "        advantages = (targets - values).reshape(-1)\n",
        "\n",
        "        # calculate loss\n",
        "        entropy = (- (probs * (probs + 1e-9).log()).sum(-1)).mean()\n",
        "        loss_V = F.smooth_l1_loss(values, targets)\n",
        "\n",
        "        index = torch.arange(0, len(probs), device = self.device)\n",
        "        actions = torch.flatten(torch.tensor(actions, device = self.device, dtype = torch.int64))\n",
        "        loss_P = -((probs[index, actions] + 1e-9).log() * advantages.detach()).mean()\n",
        "\n",
        "        loss = - entropy * self.entropy_coef + loss_V * self.V_coef + loss_P\n",
        "        loss.backward()\n",
        "\n",
        "        # norm gradient and update agent\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.update_loss_statis(loss_P.item(), loss_V.item(), entropy.item(), loss.item())\n",
        "\n",
        "    def train(self):\n",
        "        episode_reward = [0] * self.num_envs\n",
        "        episode_step = [0] * self.num_envs\n",
        "        max_episode_reward = 0\n",
        "        max_episode_step = 0\n",
        "        episode_time = [datetime.now() for _ in range(self.num_envs)]\n",
        "        total_time = datetime.now()\n",
        "\n",
        "        last_episode_rewards = []\n",
        "\n",
        "        # reset envs\n",
        "        states = self.envs.reset()\n",
        "\n",
        "        # create h, c as zeros\n",
        "        h = torch.zeros((self.num_envs, 512), dtype=torch.float, device = self.device)\n",
        "        c = torch.zeros((self.num_envs, 512), dtype=torch.float, device = self.device)\n",
        "\n",
        "        while True:\n",
        "            # finish training if agent reach total_step or total_episode based on what type of total_step_or_episode is step or episode\n",
        "            self.current_step += 1\n",
        "\n",
        "            if self.total_step_or_episode == 'step':\n",
        "                if self.current_step >= self.total_step:\n",
        "                    break\n",
        "            else:\n",
        "                if self.current_episode >= self.total_episode:\n",
        "                    break\n",
        "\n",
        "            actions, logit, V, h, c = self.select_action(states, h, c)\n",
        "\n",
        "            next_states, rewards, dones, truncs, infos = self.envs.step(actions)\n",
        "\n",
        "            # save to memory\n",
        "            self.memory.save(states, actions, rewards, next_states, dones, logit, V)\n",
        "\n",
        "            episode_reward = [x + reward for x, reward in zip(episode_reward, rewards)]\n",
        "            episode_step = [x+1 for x in episode_step]\n",
        "\n",
        "            # logging after each step, if 1 episode is ending, I will log this to logging.txt\n",
        "            for i, done in enumerate(dones):\n",
        "                if done:\n",
        "                    self.current_episode += 1\n",
        "                    max_episode_reward = max(max_episode_reward, episode_reward[i])\n",
        "                    max_episode_step = max(max_episode_step, episode_step[i])\n",
        "                    last_episode_rewards.append(episode_reward[i])\n",
        "                    f_out = open(f\"logging.txt\", \"a\")\n",
        "                    f_out.write(f'episode: {self.current_episode} agent: {i} rewards: {episode_reward[i]:.4f} steps: {episode_step[i]} complete: {infos[i][\"flag_get\"]==True} mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean():.4f} max_rewards: {max_episode_reward:.4f} max_steps: {max_episode_step} current_step: {self.current_step} loss_p: {(self.P_loss.sum()/self.len_loss):.4f} loss_v: {(self.V_loss.sum()/self.len_loss):.4f} loss_e: {(self.E_loss.sum()/self.len_loss):.4f} loss: {(self.total_loss.sum()/self.len_loss):.4f} episode_time: {datetime.now() - episode_time[i]} total_time: {datetime.now() - total_time}\\n')\n",
        "                    f_out.close()\n",
        "                    episode_reward[i] = 0\n",
        "                    episode_step[i] = 0\n",
        "                    episode_time[i] = datetime.now()\n",
        "\n",
        "            # reset h and c to zeros for enviroments that just ending episode, just multiply h and c with (1-dones)\n",
        "            h = h * (1 - torch.tensor(dones, device = self.device, dtype = torch.float).reshape(-1, 1))\n",
        "            c = c * (1 - torch.tensor(dones, device = self.device, dtype = torch.float).reshape(-1, 1))\n",
        "\n",
        "            # training agent every learn_step\n",
        "            if self.current_step % self.learn_step == 0:\n",
        "                self.learn(h, c)\n",
        "                self.memory.reset()\n",
        "                h = h.detach()\n",
        "                c = c.detach()\n",
        "\n",
        "            if self.current_step % self.save_model_step == 0:\n",
        "                self.save_model()\n",
        "\n",
        "            # eval agent every save_figure_step\n",
        "            if self.current_step % self.save_figure_step == 0 and self.save_figure_step != -1:\n",
        "                self.save_figure(is_training=True)\n",
        "                if self.is_completed:\n",
        "                    f_out = open(f\"logging.txt\", \"a\")\n",
        "                    f_out.write(f' mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean()} max_rewards: {max_episode_reward} max_steps: {max_episode_step} current_step: {self.current_step} total_time: {datetime.now() - total_time}\\n')\n",
        "                    f_out.close()\n",
        "                    return\n",
        "\n",
        "            states = list(next_states)\n",
        "\n",
        "        f_out = open(f\"logging.txt\", \"a\")\n",
        "        f_out.write(f' mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean()} max_rewards: {max_episode_reward} max_steps: {max_episode_step} current_step: {self.current_step} total_time: {datetime.now() - total_time}\\n')\n",
        "        f_out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718b77f8-186a-4f82-b7e9-330d7ac8098e",
      "metadata": {
        "id": "718b77f8-186a-4f82-b7e9-330d7ac8098e"
      },
      "source": [
        "# Create model\n",
        "I use the same model architecture as [github/uvipen/Super-mario-bros-A3C-pytorch/model](https://github.com/uvipen/Super-mario-bros-A3C-pytorch/blob/master/src/model.py).\n",
        "Model includes:\n",
        "- 4 convolution layers to encode input image (observation) to feature vector.\n",
        "- LSTM Cell to capture past observations.\n",
        "- two linear layers for policy and value prediction (actor and critic)\n",
        "\n",
        "To understand how LSTM (recurrent network) works in RL, we can view (h, c) from the lastest step as input of the model. Then the model takes 3 inputs: (state, h, c) instead of just state as a non-recurrent model. Then we just save output (h, c) for each state and reuse it as input with (next_state, h, c).\n",
        "\n",
        "After completing this project, I just tried to remove _initialize_weights and performance did not change in some stages, then I thought _initialize_weights is not necessary, but I don't want to remove it because I am not sure this code works for all stages without _initialize_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee1e253-b83d-48d5-aa9c-4463303c3feb",
      "metadata": {
        "id": "dee1e253-b83d-48d5-aa9c-4463303c3feb"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, detach_lstm_state, init_weights):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_dim[0], 32, 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.lstm = nn.LSTMCell(1152, 512)\n",
        "        self.critic_linear = nn.Linear(512, 1)\n",
        "        self.actor_linear = nn.Linear(512, output_dim)\n",
        "        self.detach_lstm_state = detach_lstm_state\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                # nn.init.kaiming_uniform_(module.weight)\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.LSTMCell):\n",
        "                nn.init.constant_(module.bias_ih, 0)\n",
        "                nn.init.constant_(module.bias_hh, 0)\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        x = F.relu(self.conv1(x/255.))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        h, c = self.lstm(x, (h, c))\n",
        "        if self.detach_lstm_state:\n",
        "            h = h.detach()\n",
        "            c = c.detach()\n",
        "        return self.actor_linear(h), self.critic_linear(h), h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a7606a-b416-4c17-a3c9-85a6ce07b508",
      "metadata": {
        "id": "c2a7606a-b416-4c17-a3c9-85a6ce07b508"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67741e59",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model(config.state_dim, config.action_dim, config.detach_lstm_state, config.init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077a241d-419a-4f72-9f2c-7356d7858eae",
      "metadata": {
        "id": "077a241d-419a-4f72-9f2c-7356d7858eae"
      },
      "outputs": [],
      "source": [
        "envs = MultipleEnvironments(config.world, config.stage, config.action_type, config.num_envs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4637f85-679c-4034-befe-14d839485d47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4637f85-679c-4034-befe-14d839485d47",
        "outputId": "f7cea27e-9d8a-43b7-9843-8d7ccebe9ab7"
      },
      "outputs": [],
      "source": [
        "envs.envs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4f5fde-6390-4a81-bb29-223fa70e9539",
      "metadata": {
        "id": "ae4f5fde-6390-4a81-bb29-223fa70e9539"
      },
      "outputs": [],
      "source": [
        "agent = Agent(world = config.world, stage = config.stage, action_type = config.action_type, envs = envs, num_envs = config.num_envs, \n",
        "              state_dim = config.state_dim, action_dim = config.action_dim, save_dir = config.save_dir,\n",
        "              save_model_step = config.save_model_step, save_figure_step = config.save_figure_step, learn_step = config.learn_step,\n",
        "              total_step_or_episode = config.total_step_or_episode, total_step = config.total_step, total_episode = config.total_episode,\n",
        "              model = model, gamma = config.gamma, learning_rate = config.learning_rate, entropy_coef = config.entropy_coef, V_coef = config.V_coef,\n",
        "              max_grad_norm = config.max_grad_norm, optimizer_eps = config.optimizer_eps,\n",
        "              device = \"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f94e0c8-e8c7-4f37-ae4b-ffc9083b6aca",
      "metadata": {
        "id": "5f94e0c8-e8c7-4f37-ae4b-ffc9083b6aca"
      },
      "outputs": [],
      "source": [
        "agent.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NLjlwqf7qdZK",
      "metadata": {
        "id": "NLjlwqf7qdZK"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4hrG4FCepxby",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hrG4FCepxby",
        "outputId": "2a27355a-65f5-49cb-a218-5de23e15e58c"
      },
      "outputs": [],
      "source": [
        "agent.load_model(\"best_model.pth\")\n",
        "agent.save_figure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EvVY0vqQqFJB",
      "metadata": {
        "id": "EvVY0vqQqFJB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
