{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19939a9c-03f0-4150-9477-9109e9735319",
      "metadata": {
        "id": "19939a9c-03f0-4150-9477-9109e9735319"
      },
      "source": [
        "# install packages\n",
        "install packages need to train mario agent. Note:\n",
        "- code will bug with later numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a21d2d-dccd-447b-84e8-37c55db5ef2d",
      "metadata": {
        "id": "44a21d2d-dccd-447b-84e8-37c55db5ef2d"
      },
      "outputs": [],
      "source": [
        "# !pip install torch\n",
        "# !pip install matplotlib\n",
        "!pip install gymnasium==0.29.1\n",
        "!pip install gym-super-mario-bros==7.4.0\n",
        "!pip install gym==0.25.2\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install imageio\n",
        "# !pip install torchvision\n",
        "!pip install opencv-python-headless\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829519d4-5bf4-46ed-8b52-8f03fee2f8d9",
      "metadata": {
        "id": "829519d4-5bf4-46ed-8b52-8f03fee2f8d9"
      },
      "source": [
        "# import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7144ed99-8f5f-489c-b6b4-2ce51f9b9d8f",
      "metadata": {
        "id": "7144ed99-8f5f-489c-b6b4-2ce51f9b9d8f"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import copy\n",
        "import cv2\n",
        "import imageio\n",
        "import numpy as np\n",
        "import random, os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "#import multiprocessing as mp\n",
        "from torchvision import transforms as T\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd03a2e1-8ccd-4dd5-a13d-87f265c1c55e",
      "metadata": {
        "id": "bd03a2e1-8ccd-4dd5-a13d-87f265c1c55e"
      },
      "source": [
        "# Create hyperparammeters\n",
        "Config hyperparammeters, just change it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a95999c-5111-4054-9256-45cd206a697e",
      "metadata": {
        "id": "4a95999c-5111-4054-9256-45cd206a697e"
      },
      "outputs": [],
      "source": [
        "#class DictWrapper created by Chatgpt\n",
        "class DictWrapper:\n",
        "    def __init__(self, dictionary):\n",
        "        self._dict = dictionary\n",
        "\n",
        "    def __getattr__(self, item):\n",
        "        if item in self._dict:\n",
        "            return self._dict[item]\n",
        "        else:\n",
        "            raise AttributeError(f\"'DictWrapper' object has no attribute '{item}'\")\n",
        "\n",
        "config = {\n",
        "    'num_envs': 32,\n",
        "    'save_model_step': int(1e5),\n",
        "    'save_figure_step': 500,\n",
        "    'learn_step': 20,\n",
        "    'total_step_or_episode': 'step',\n",
        "    'total_step': int(5e6),\n",
        "    'total_episode': None,\n",
        "    'save_dir': \"\",\n",
        "    'gamma': 0.99,\n",
        "    'learning_rate': 5e-4,\n",
        "    'state_dim': (4, 84, 84),\n",
        "    'action_dim': 12, #12 for complex, 7 for simple\n",
        "    'entropy_coef': 0.01,\n",
        "    'V_coef': 0.5,\n",
        "    'world': 1,\n",
        "    'stage': 1,\n",
        "    'action_type': 'complex',\n",
        "    'init_weights': True, #use _initialize_weights function or not\n",
        "    'kfac_momentum': 0.9,\n",
        "    'stat_decay': 0.99,\n",
        "    'kfac_Ts': 1,\n",
        "    'kfac_Tf': 10,\n",
        "    'kfac_kl_clip': 0.001,\n",
        "    'kfac_damping': 1e-2,\n",
        "    'kfac_fast_cnn': False,\n",
        "    'weight_decay': 0,\n",
        "    'gae_lambda': 0.95,\n",
        "    'eigen_eps': 1e-4 #This parameter can distort the gradient, but it helps the algorithm not to fail when calculating the eigenvalues. Reducing this parameter may make the agent better (no guarantees).\n",
        "}\n",
        "\n",
        "config = DictWrapper(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ha80HrVm1_OR",
      "metadata": {
        "id": "ha80HrVm1_OR"
      },
      "outputs": [],
      "source": [
        "gym.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "440a3f9c-d1fd-41e5-a431-592e719aef84",
      "metadata": {
        "id": "440a3f9c-d1fd-41e5-a431-592e719aef84"
      },
      "source": [
        "# Define environment\n",
        "## Create a custom environment, We need:\n",
        "- SkipFrame: Because the episode is very long, we only need to repeat actions sometimes in this environment. We repeat each action 4 times and skip the first 3 frames (returning the 4th frame). We also sum the rewards from all 4 frames.\n",
        "- GrayScaleResizeObservation: Convert the state to grayscale (from RGB to a gray image) and resize it to 84x84 pixels.\n",
        "- NoopResetEnv: When resetting the environment, we perform random actions before starting the environment. This is similar to the Atari strategy. When resetting, we randomly choose num_noops actions between 0 and noop_max and perform num_noops random actions. If the random actions lead to a terminal state, we reset and continue performing random actions. I set noop_max to 30, similar to Atari.\n",
        "- CustomRewardAndDoneEnv\n",
        "    - I noticed that many people train Mario using this custom reward system, so I copied it. The system adds 50 reward points if the agent solves the stage and subtracts 50 reward points if the agent dies. The reward is divided by 10. I set done to True if Mario dies, instead of the default setting where Mario loses all lives.\n",
        "    - Stage 4-2: Subtract 50 reward points if Mario moves on top of the map (y_pos >= 255).\n",
        "    - Stages 4-4 and 7-4: Set done = True when Mario goes the wrong way and subtract 50 reward points as a penalty. If Mario takes the correct path but the map still loops (a known bug), I set done = True but do not apply a penalty.\n",
        "    - Stages 4-4: give a -0.1 reward every step.\n",
        "\n",
        "\n",
        "## About reward system:\n",
        "- Set done to True when Mario dies: This is the most important aspect because, in the default reward system, Mario still gains a reward by just moving right. If Mario dies, the agent doesn't lose total rewards and can continue moving right (in the new life) to get more rewards. This is the easiest way for the agent to earn rewards, and it can learn to exploit this trick.\n",
        "- Penalty of -50 reward when Mario dies: This is necessary to speed up Mario's training. Without this penalty, the agent may struggle to complete more difficult stages.\n",
        "- Reward of 50 when reaching the flag: This encourages Mario to train faster and overcome difficult sections in harder stages.\n",
        "- Changing the penalty and flag reward to more or less than 50 doesn't make a significant difference, so I haven't changed it.\n",
        "- Divide rewards by 10: I believe this reduces the total rewards and helps the agent learn a better strategy, but I'm not entirely sure how necessary this is. I simply followed an existing approach.\n",
        "- With Stage 4-2: I noticed that the agent can earn more rewards when Mario goes to the warp zone, but Mario can't win this stage using the warp zone because the reward system gives negative rewards when Mario moves left. Therefore, I added a penalty when Mario moves to the top of the map.\n",
        "- Use FrameStack to stack the latest 4 frames as observation.\n",
        "- With Stage 4-4 and 7-4:\n",
        "    - Since this map has a wrong path, Mario can enter a loop where the reward increases indefinitely. To prevent this, I set done = True and assign a negative penalty reward.\n",
        "    - I also give a negative reward to prevent Mario from taking the wrong path.\n",
        "    - Another strategy is to give a negative reward without setting done = True (as in 4-2). However, this strategy doesn't work due to a bug in this map.\n",
        "    - Even when Mario is on the correct path, sometimes he still enters the loop. To handle this, I set done = True every time Mario enters the loop (checked by x_pos and max_x_pos).\n",
        "- Stage 4-4: Assign a -0.1 reward for every step: This prevents Mario from getting stuck. This map has a section where Mario needs to move left, but moving left incurs a negative reward in the default system. If Mario moves right, he takes the wrong path, causing the episode to end with a negative reward. To keep Mario moving, I added a negative reward for every step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba268b64-6d59-4682-ac6e-f4118bd2d6da",
      "metadata": {
        "id": "ba268b64-6d59-4682-ac6e-f4118bd2d6da"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, trunk, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, trunk, info\n",
        "\n",
        "class GrayScaleResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.current_state = observation\n",
        "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
        "        observation = observation.astype(np.uint8)#.reshape(-1, observation.shape[0], observation.shape[1])\n",
        "        return observation\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        super(NoopResetEnv, self).__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        obs = self.env.reset(**kwargs)\n",
        "        noops = np.random.randint(0, self.noop_max, (1, ))[0]\n",
        "        for _ in range(noops):\n",
        "            action = self.env.action_space.sample()\n",
        "            obs, _, done, _, _ = self.env.step(action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        obs, reward, done, trunk, info = self.env.step(ac)\n",
        "        return obs, reward, done, trunk, info\n",
        "\n",
        "class CustomRewardAndDoneEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, world=1, stage=1, additional_bonus_state_8_4_option = \"no\"):\n",
        "        super(CustomRewardAndDoneEnv, self).__init__(env)\n",
        "        self.current_score = 0\n",
        "        self.current_x = 0\n",
        "        self.old_x = -1\n",
        "        self.current_x_count = 0\n",
        "        self.max_x = 0\n",
        "        self.world = world\n",
        "        self.stage = stage\n",
        "        if self.world == 8 and self.stage == 4:\n",
        "            self.sea_map = False\n",
        "            self.additional_bonus_state_8_4_option = additional_bonus_state_8_4_option\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.current_score = 0\n",
        "        self.current_x = 0\n",
        "        self.old_x = -1\n",
        "        self.current_x_count = 0\n",
        "        self.max_x = 0\n",
        "        if self.world == 8 and self.stage == 4:\n",
        "            self.sea_map = False\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done, trunc, info = self.env.step(action)\n",
        "\n",
        "        if (info['x_pos'] - self.current_x) == 0:\n",
        "            self.current_x_count += 1\n",
        "        else:\n",
        "            self.current_x_count = 0\n",
        "        if info[\"flag_get\"]:\n",
        "            reward += 50\n",
        "            done = True\n",
        "        if done and info[\"flag_get\"] == False and info[\"time\"] != 0:\n",
        "            reward -= 50\n",
        "            done = True\n",
        "        self.current_x = info[\"x_pos\"]\n",
        "\n",
        "        if self.world == 7 and self.stage == 4:\n",
        "            if (506 <= info[\"x_pos\"] <= 832 and info[\"y_pos\"] > 127) or (\n",
        "                    832 < info[\"x_pos\"] <= 1064 and info[\"y_pos\"] < 80) or (\n",
        "                    1113 < info[\"x_pos\"] <= 1464 and info[\"y_pos\"] < 191) or (\n",
        "                    1579 < info[\"x_pos\"] <= 1943 and info[\"y_pos\"] < 191) or (\n",
        "                    1946 < info[\"x_pos\"] <= 1964 and info[\"y_pos\"] >= 191) or (\n",
        "                    1984 < info[\"x_pos\"] <= 2060 and (info[\"y_pos\"] >= 191 or info[\"y_pos\"] < 127)) or (\n",
        "                    2114 < info[\"x_pos\"] < 2440 and info[\"y_pos\"] < 191):\n",
        "                reward -= 50\n",
        "                done = True\n",
        "            if done == False and info[\"x_pos\"] < self.max_x - 100:\n",
        "                done = True\n",
        "        if self.world == 4 and self.stage == 4:\n",
        "            if (info[\"x_pos\"] <= 1500 and info[\"y_pos\"] < 127) or (\n",
        "                    1588 <= info[\"x_pos\"] < 2380 and info[\"y_pos\"] >= 127):\n",
        "                reward -= 50\n",
        "                done = True\n",
        "            if done == False and info[\"x_pos\"] < self.max_x - 100:\n",
        "                done = True\n",
        "            if done == False:\n",
        "                reward -= 0.1\n",
        "        if self.world == 4 and self.stage == 2 and done == False and info['y_pos'] >= 255:\n",
        "            reward -= 50\n",
        "        if self.world == 8 and self.stage == 4:\n",
        "            if info[\"x_pos\"] > 2440 and info[\"x_pos\"] <= 2500:\n",
        "                done = True\n",
        "                reward -= 100\n",
        "            if info[\"x_pos\"] >= 3675 and info[\"x_pos\"] <= 3700:\n",
        "                done = True\n",
        "                reward -= 50\n",
        "\n",
        "            if info[\"x_pos\"] < self.max_x - 200:\n",
        "                if self.max_x >= 1240 and self.max_x <= 1310: #solved bug because x_pos duplicated\n",
        "                    if info[\"x_pos\"] >= 320:\n",
        "                        done = True\n",
        "                        reward -= 50\n",
        "\n",
        "            if info[\"x_pos\"] < self.old_x - 200:\n",
        "                if info[\"x_pos\"] >= 312-5 and info[\"x_pos\"] <= 312+5:\n",
        "                    done = True\n",
        "                    reward -= 50\n",
        "                elif info[\"x_pos\"] >= 56-5 and info[\"x_pos\"] <= 56+5 and self.max_x > 3645 and self.sea_map == False:\n",
        "                    if self.additional_bonus_state_8_4_option == 'right_pipe':\n",
        "                        reward += 50\n",
        "                    self.sea_map = True\n",
        "            if self.additional_bonus_state_8_4_option == 'right_pipe':\n",
        "                if info[\"x_pos\"] > self.max_x + 100:\n",
        "                    reward += 50\n",
        "            if done == False:\n",
        "                reward -= 0.1\n",
        "        self.max_x = max(self.max_x, self.current_x)\n",
        "        self.current_score = info[\"score\"]\n",
        "        self.old_x = self.current_x\n",
        "\n",
        "        return state, reward / 10., done, trunc, info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec30b8e-1b8d-487f-98f3-21615ce704ee",
      "metadata": {
        "id": "6ec30b8e-1b8d-487f-98f3-21615ce704ee"
      },
      "source": [
        "# Create MultipleEnvironments\n",
        "MultipleEnvironments use multi-processing to parallel running.\n",
        "\n",
        "Because in the training process, we need to reset the environment when the agent reaches the terminal state. But if we will do it in parallel, then I don't want to check each environment and reset (by loop) or create a new function that parallels check and reset all environments. Then I reset the environment if done = True in step function and set next_state = env.reset(). Then in training, we just set state = next_state (next_state is reset state if done = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y2mcbU_JvQwI",
      "metadata": {
        "id": "y2mcbU_JvQwI"
      },
      "outputs": [],
      "source": [
        "#modify from https://github.com/uvipen/Super-mario-bros-PPO-pytorch/blob/master/src/env.py\n",
        "def create_env(world, stage, action_type, test=False):\n",
        "    if gym.__version__ < '0.26':\n",
        "        env = gym_super_mario_bros.make(f\"SuperMarioBros-{world}-{stage}-v0\", new_step_api=True)\n",
        "    else:\n",
        "        env = gym_super_mario_bros.make(f\"SuperMarioBros-{world}-{stage}-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "    if action_type == \"right\":\n",
        "        action_type = RIGHT_ONLY\n",
        "    elif action_type == \"simple\":\n",
        "        action_type = SIMPLE_MOVEMENT\n",
        "    else:\n",
        "        action_type = COMPLEX_MOVEMENT\n",
        "\n",
        "    env = JoypadSpace(env, action_type)\n",
        "\n",
        "    if test == False:\n",
        "        env = NoopResetEnv(env)\n",
        "    env = SkipFrame(env, skip=4)\n",
        "    env = CustomRewardAndDoneEnv(env, world, stage)\n",
        "    env = GrayScaleResizeObservation(env, shape=84)\n",
        "    if gym.__version__ < '0.26':\n",
        "        env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "    else:\n",
        "        env = FrameStack(env, num_stack=4)\n",
        "    return env\n",
        "\n",
        "class MultipleEnvironments:\n",
        "    def __init__(self, world, stage, action_type, num_envs):\n",
        "        self.agent_conns, self.env_conns = zip(*[mp.Pipe(duplex=True) for _ in range(num_envs)])\n",
        "        self.envs = [create_env(world, stage, action_type) for _ in range(num_envs)]\n",
        "\n",
        "        for index in range(num_envs):\n",
        "            process = mp.Process(target=self.run, args=(index,))\n",
        "            process.start()\n",
        "            self.env_conns[index].close()\n",
        "\n",
        "    def run(self, index):\n",
        "        self.agent_conns[index].close()\n",
        "        while True:\n",
        "            request, action = self.env_conns[index].recv()\n",
        "            if request == \"step\":\n",
        "                next_state, reward, done, trunc, info = self.envs[index].step(action)\n",
        "                if done:\n",
        "                    next_state = self.envs[index].reset()\n",
        "                self.env_conns[index].send((next_state, reward, done, trunc, info))\n",
        "            elif request == \"reset\":\n",
        "                self.env_conns[index].send(self.envs[index].reset())\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "    def step(self, actions):\n",
        "        [agent_conn.send((\"step\", act)) for agent_conn, act in zip(self.agent_conns, actions)]\n",
        "        next_states, rewards, dones, truncs, infos = zip(*[agent_conn.recv() for agent_conn in self.agent_conns])\n",
        "        return next_states, rewards, dones, truncs, infos\n",
        "\n",
        "    def reset(self):\n",
        "        [agent_conn.send((\"reset\", None)) for agent_conn in self.agent_conns]\n",
        "        states = [agent_conn.recv() for agent_conn in self.agent_conns]\n",
        "        return states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31dycPWmOuOR",
      "metadata": {
        "id": "31dycPWmOuOR"
      },
      "outputs": [],
      "source": [
        "mp.cpu_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a894f8-f303-4fac-b478-0b191f38f274",
      "metadata": {
        "id": "92a894f8-f303-4fac-b478-0b191f38f274"
      },
      "source": [
        "# Create memory\n",
        "Memory just save all info we need to train and return all stored info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1736a4f8-d863-4bed-b290-3812f6c43eae",
      "metadata": {
        "id": "1736a4f8-d863-4bed-b290-3812f6c43eae"
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "    def __init__(self, num_envs):\n",
        "        self.num_envs = num_envs\n",
        "\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.next_states = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.logits = []\n",
        "        self.values = []\n",
        "\n",
        "    def save(self, state, action, reward, next_state, done, logit, V):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.next_states.append(next_state)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.logits.append(logit)\n",
        "        self.values.append(V)\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.next_states = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.logits = []\n",
        "        self.values = []\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.states, self.actions, self.next_states, self.rewards, self.dones, self.logits, self.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TqX8gx8GUClj",
      "metadata": {
        "id": "TqX8gx8GUClj"
      },
      "source": [
        "# KFAC\n",
        "Edit from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/algo/kfac.py\n",
        "\n",
        "I changed torch.symeig to torch.linalg.eigh because the new version of torch does not support torch.symeig and torch.symeig is reported to have a bug when calculating gradient.\n",
        "\n",
        "I added the eigen_eps parameter to make torch.linalg.eigh work. Without eigen_eps, the algorithm will have one of three errors:\n",
        "\n",
        "- The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues\n",
        "- This error may appear if the input matrix contains NaN. (nan model)\n",
        "- Stop without an obvious error (possibly because eigenvalues < 1e-6 leading to no gradient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W0eOZFV3qoDx",
      "metadata": {
        "id": "W0eOZFV3qoDx"
      },
      "outputs": [],
      "source": [
        "#Edit from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/algo/kfac.py\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Necessary for my KFAC implementation.\n",
        "class AddBias(nn.Module):\n",
        "    def __init__(self, bias):\n",
        "        super(AddBias, self).__init__()\n",
        "        self._bias = nn.Parameter(bias.unsqueeze(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 2:\n",
        "            bias = self._bias.t().view(1, -1)\n",
        "        else:\n",
        "            bias = self._bias.t().view(1, -1, 1, 1)\n",
        "\n",
        "        return x + bias\n",
        "\n",
        "# TODO: In order to make this code faster:\n",
        "# 1) Implement _extract_patches as a single cuda kernel\n",
        "# 2) Compute QR decomposition in a separate process\n",
        "# 3) Actually make a general KFAC optimizer so it fits PyTorch\n",
        "\n",
        "\n",
        "def _extract_patches(x, kernel_size, stride, padding):\n",
        "    if padding[0] + padding[1] > 0:\n",
        "        x = F.pad(x, (padding[1], padding[1], padding[0],\n",
        "                      padding[0])).data  # Actually check dims\n",
        "    x = x.unfold(2, kernel_size[0], stride[0])\n",
        "    x = x.unfold(3, kernel_size[1], stride[1])\n",
        "    x = x.transpose_(1, 2).transpose_(2, 3).contiguous()\n",
        "    x = x.view(\n",
        "        x.size(0), x.size(1), x.size(2),\n",
        "        x.size(3) * x.size(4) * x.size(5))\n",
        "    return x\n",
        "\n",
        "\n",
        "def compute_cov_a(a, classname, layer_info, fast_cnn):\n",
        "    batch_size = a.size(0)\n",
        "\n",
        "    if classname == 'Conv2d':\n",
        "        if fast_cnn:\n",
        "            a = _extract_patches(a, *layer_info)\n",
        "            a = a.view(a.size(0), -1, a.size(-1))\n",
        "            a = a.mean(1)\n",
        "        else:\n",
        "            a = _extract_patches(a, *layer_info)\n",
        "            a = a.view(-1, a.size(-1)).div_(a.size(1)).div_(a.size(2))\n",
        "    elif classname == 'AddBias':\n",
        "        is_cuda = a.is_cuda\n",
        "        a = torch.ones(a.size(0), 1)\n",
        "        if is_cuda:\n",
        "            a = a.cuda()\n",
        "\n",
        "    return a.t() @ (a / batch_size)\n",
        "\n",
        "\n",
        "def compute_cov_g(g, classname, layer_info, fast_cnn):\n",
        "    batch_size = g.size(0)\n",
        "\n",
        "    if classname == 'Conv2d':\n",
        "        if fast_cnn:\n",
        "            g = g.view(g.size(0), g.size(1), -1)\n",
        "            g = g.sum(-1)\n",
        "        else:\n",
        "            g = g.transpose(1, 2).transpose(2, 3).contiguous()\n",
        "            g = g.view(-1, g.size(-1)).mul_(g.size(1)).mul_(g.size(2))\n",
        "    elif classname == 'AddBias':\n",
        "        g = g.view(g.size(0), g.size(1), -1)\n",
        "        g = g.sum(-1)\n",
        "\n",
        "    g_ = g * batch_size\n",
        "    return g_.t() @ (g_ / g.size(0))\n",
        "\n",
        "\n",
        "def update_running_stat(aa, m_aa, momentum):\n",
        "    # Do the trick to keep aa unchanged and not create any additional tensors\n",
        "    m_aa *= momentum / (1 - momentum)\n",
        "    m_aa += aa\n",
        "    m_aa *= (1 - momentum)\n",
        "\n",
        "\n",
        "class SplitBias(nn.Module):\n",
        "    def __init__(self, module):\n",
        "        super(SplitBias, self).__init__()\n",
        "        self.module = module\n",
        "        self.add_bias = AddBias(module.bias.data)\n",
        "        self.module.bias = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.module(input)\n",
        "        x = self.add_bias(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class KFACOptimizer(optim.Optimizer):\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 lr=0.25,\n",
        "                 momentum=0.9,\n",
        "                 stat_decay=0.99,\n",
        "                 kl_clip=0.001,\n",
        "                 damping=1e-2,\n",
        "                 weight_decay=0,\n",
        "                 fast_cnn=False,\n",
        "                 Ts=1,\n",
        "                 Tf=10,\n",
        "                 eigen_eps=1e-4):\n",
        "        defaults = dict()\n",
        "\n",
        "        def split_bias(module):\n",
        "            for mname, child in module.named_children():\n",
        "                if hasattr(child, 'bias') and child.bias is not None:\n",
        "                    module._modules[mname] = SplitBias(child)\n",
        "                else:\n",
        "                    split_bias(child)\n",
        "\n",
        "        split_bias(model)\n",
        "\n",
        "        super(KFACOptimizer, self).__init__(model.parameters(), defaults)\n",
        "\n",
        "        self.known_modules = {'Linear', 'Conv2d', 'AddBias'}\n",
        "\n",
        "        self.modules = []\n",
        "        self.grad_outputs = {}\n",
        "\n",
        "        self.model = model\n",
        "        self._prepare_model()\n",
        "\n",
        "        self.steps = 0\n",
        "\n",
        "        self.m_aa, self.m_gg = {}, {}\n",
        "        self.Q_a, self.Q_g = {}, {}\n",
        "        self.d_a, self.d_g = {}, {}\n",
        "\n",
        "        self.momentum = momentum\n",
        "        self.stat_decay = stat_decay\n",
        "\n",
        "        self.lr = lr\n",
        "        self.kl_clip = kl_clip\n",
        "        self.damping = damping\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        self.fast_cnn = fast_cnn\n",
        "\n",
        "        self.Ts = Ts\n",
        "        self.Tf = Tf\n",
        "        self.eigen_eps = eigen_eps\n",
        "\n",
        "        self.optim = optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=self.lr * (1 - self.momentum),\n",
        "            momentum=self.momentum)\n",
        "\n",
        "    def _save_input(self, module, input):\n",
        "        if torch.is_grad_enabled() and self.steps % self.Ts == 0:\n",
        "            classname = module.__class__.__name__\n",
        "            layer_info = None\n",
        "            if classname == 'Conv2d':\n",
        "                layer_info = (module.kernel_size, module.stride,\n",
        "                              module.padding)\n",
        "\n",
        "            aa = compute_cov_a(input[0].data, classname, layer_info,\n",
        "                               self.fast_cnn)\n",
        "\n",
        "            # Initialize buffers\n",
        "            if self.steps == 0:\n",
        "                self.m_aa[module] = aa.clone()\n",
        "\n",
        "            update_running_stat(aa, self.m_aa[module], self.stat_decay)\n",
        "\n",
        "    def _save_grad_output(self, module, grad_input, grad_output):\n",
        "        # Accumulate statistics for Fisher matrices\n",
        "        if self.acc_stats:\n",
        "            classname = module.__class__.__name__\n",
        "            layer_info = None\n",
        "            if classname == 'Conv2d':\n",
        "                layer_info = (module.kernel_size, module.stride,\n",
        "                              module.padding)\n",
        "\n",
        "            gg = compute_cov_g(grad_output[0].data, classname, layer_info,\n",
        "                               self.fast_cnn)\n",
        "\n",
        "            # Initialize buffers\n",
        "            if self.steps == 0:\n",
        "                self.m_gg[module] = gg.clone()\n",
        "\n",
        "            update_running_stat(gg, self.m_gg[module], self.stat_decay)\n",
        "\n",
        "    def _prepare_model(self):\n",
        "        for module in self.model.modules():\n",
        "            classname = module.__class__.__name__\n",
        "            if classname in self.known_modules:\n",
        "                assert not ((classname in ['Linear', 'Conv2d']) and module.bias is not None), \\\n",
        "                                    \"You must have a bias as a separate layer\"\n",
        "\n",
        "                self.modules.append(module)\n",
        "                module.register_forward_pre_hook(self._save_input)\n",
        "                module.register_backward_hook(self._save_grad_output)\n",
        "\n",
        "    def step(self):\n",
        "        # Add weight decay\n",
        "        if self.weight_decay > 0:\n",
        "            for p in self.model.parameters():\n",
        "                p.grad.data.add_(self.weight_decay, p.data)\n",
        "\n",
        "        updates = {}\n",
        "        for i, m in enumerate(self.modules):\n",
        "            assert len(list(m.parameters())\n",
        "                       ) == 1, \"Can handle only one parameter at the moment\"\n",
        "            classname = m.__class__.__name__\n",
        "            p = next(m.parameters())\n",
        "\n",
        "            la = self.damping + self.weight_decay\n",
        "\n",
        "            if self.steps % self.Tf == 0:\n",
        "                # My asynchronous implementation exists, I will add it later.\n",
        "                # Experimenting with different ways to this in PyTorch.\n",
        "                # self.d_a[m], self.Q_a[m] = torch.symeig(\n",
        "                #     self.m_aa[m], eigenvectors=True)\n",
        "                # self.d_g[m], self.Q_g[m] = torch.symeig(\n",
        "                #     self.m_gg[m], eigenvectors=True)\n",
        "                self.d_a[m], self.Q_a[m] = torch.linalg.eigh(self.m_aa[m] + self.eigen_eps * torch.eye(self.m_aa[m].size(0), device=self.m_aa[m].device), \"U\")\n",
        "                self.d_g[m], self.Q_g[m] = torch.linalg.eigh(self.m_gg[m] + self.eigen_eps * torch.eye(self.m_gg[m].size(0), device=self.m_gg[m].device), \"U\")\n",
        "\n",
        "                self.d_a[m].mul_((self.d_a[m] > 1e-6).float())\n",
        "                self.d_g[m].mul_((self.d_g[m] > 1e-6).float())\n",
        "\n",
        "            if classname == 'Conv2d':\n",
        "                p_grad_mat = p.grad.data.view(p.grad.data.size(0), -1)\n",
        "            else:\n",
        "                p_grad_mat = p.grad.data\n",
        "\n",
        "            v1 = self.Q_g[m].t() @ p_grad_mat @ self.Q_a[m]\n",
        "            v2 = v1 / (\n",
        "                self.d_g[m].unsqueeze(1) * self.d_a[m].unsqueeze(0) + la)\n",
        "            v = self.Q_g[m] @ v2 @ self.Q_a[m].t()\n",
        "\n",
        "            v = v.view(p.grad.data.size())\n",
        "            updates[p] = v\n",
        "\n",
        "        vg_sum = 0\n",
        "        for p in self.model.parameters():\n",
        "            v = updates[p]\n",
        "            vg_sum += (v * p.grad.data * self.lr * self.lr).sum()\n",
        "\n",
        "        nu = min(1, math.sqrt(self.kl_clip / vg_sum))\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            v = updates[p]\n",
        "            p.grad.data.copy_(v)\n",
        "            p.grad.data.mul_(nu)\n",
        "\n",
        "        self.optim.step()\n",
        "        self.steps += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4303a12-57dc-46c5-9134-5d2e20e0438f",
      "metadata": {
        "id": "c4303a12-57dc-46c5-9134-5d2e20e0438f"
      },
      "source": [
        "# Create agent\n",
        "The agent includes 4 main functions:\n",
        "## train\n",
        "train function train agent via many episodes:\n",
        "- loop until the agent wins this stage or reaches the maximum episode/step:\n",
        "    - predict value, logit for the current state\n",
        "    - sample action from logit with category distribution (select_action function)\n",
        "    - log all info to memory\n",
        "- train agent every learn_step (learn function)\n",
        "- eval agent every save_figure_step (save_figure function)\n",
        "- set state = next_state (I reset environment when agent reach terminal state then next_state is first state if done=True)\n",
        "\n",
        "## select_action\n",
        "this function sample action from logit:\n",
        "- just convert logit to probability: policy = F.softmax(logits, dim=1)\n",
        "- create distribution from probability: distribution = torch.distributions.Categorical(policy)\n",
        "- sample action from distribution: actions = distribution.sample()\n",
        "\n",
        "## save_figure\n",
        "this function eval agent and saves agent/video if the agent yields better total rewards:\n",
        "- loop until the agent reaches the terminal state.\n",
        "    - predict logit from model\n",
        "    - get action = argmax (logit)\n",
        "    - environment do this action to get next_state, reward, info, done\n",
        "- if total_reward > best test total reward or agent complete this stage, I save model and video.\n",
        "- if agent completes this state, we stop training.\n",
        "\n",
        "## learn\n",
        "this function trains agent from the experiment saved in memory\n",
        "- get all the info from memory\n",
        "- calculate td (lambda) target and gae advantages\n",
        "- calculate loss:\n",
        "    - with KFAC: we only need add backward fisher matrix\n",
        "- update model from loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d-yL01WMumqF",
      "metadata": {
        "id": "d-yL01WMumqF"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, world, stage, action_type, envs, num_envs, state_dim, action_dim, save_dir, save_model_step,\n",
        "                 save_figure_step, learn_step, total_step_or_episode, total_step, total_episode, model,\n",
        "                 gamma, learning_rate, entropy_coef, V_coef, kfac_momentum, stat_decay, kfac_kl_clip,\n",
        "                 kfac_damping, weight_decay, kfac_fast_cnn, kfac_Ts, kfac_Tf, gae_lambda, eigen_eps, device):\n",
        "        self.world = world\n",
        "        self.stage = stage\n",
        "        self.action_type = action_type\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "        self.learn_step = learn_step\n",
        "        self.total_step_or_episode = total_step_or_episode\n",
        "        self.total_step = total_step\n",
        "        self.total_episode = total_episode\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.current_episode = 0\n",
        "\n",
        "        self.save_model_step = save_model_step\n",
        "        self.save_figure_step = save_figure_step\n",
        "\n",
        "        self.device = device\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.num_envs = num_envs\n",
        "        self.envs = envs\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.V_coef = V_coef\n",
        "        self.optimizer = KFACOptimizer(self.model, self.learning_rate, kfac_momentum, stat_decay, kfac_kl_clip,\n",
        "                                       kfac_damping, weight_decay, kfac_fast_cnn, kfac_Ts, kfac_Tf, eigen_eps)\n",
        "\n",
        "        self.memory = Memory(self.num_envs)\n",
        "\n",
        "        self.is_completed = False\n",
        "\n",
        "        self.env = None\n",
        "        self.max_test_score = -1e9\n",
        "\n",
        "        # I just log 1000 lastest update and print it to log.\n",
        "        self.V_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.P_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.E_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.total_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.loss_index = 0\n",
        "        self.len_loss = 0\n",
        "\n",
        "    def save_figure(self, is_training=False):\n",
        "        # test current model and save model/figure if model yield best total rewards.\n",
        "        # create env for testing, reset test env\n",
        "        if self.env is None:\n",
        "            self.env = create_env(self.world, self.stage, self.action_type, True)\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "\n",
        "        images = []\n",
        "        total_reward = 0\n",
        "        total_step = 0\n",
        "        num_repeat_action = 0\n",
        "        old_action = -1\n",
        "\n",
        "        episode_time = datetime.now()\n",
        "\n",
        "        # play 1 episode, just get loop action with max probability from model until the episode end.\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                logit, V = self.model(torch.tensor(state, dtype = torch.float, device = self.device).unsqueeze(0))\n",
        "            action = logit.argmax(-1).item()\n",
        "            next_state, reward, done, trunc, info = self.env.step(action)\n",
        "            state = next_state\n",
        "            img = Image.fromarray(self.env.current_state)\n",
        "            images.append(img)\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "\n",
        "            if action == old_action:\n",
        "                num_repeat_action += 1\n",
        "            else:\n",
        "                num_repeat_action = 0\n",
        "            old_action = action\n",
        "            if num_repeat_action == 200:\n",
        "                break\n",
        "\n",
        "        #logging, if model yield better result, save figure (test_episode.mp4) and model (best_model.pth)\n",
        "        if is_training:\n",
        "            f_out = open(f\"logging_test.txt\", \"a\")\n",
        "            f_out.write(f'episode_reward: {total_reward} episode_step: {total_step} current_step: {self.current_step} loss_p: {(self.P_loss.sum()/self.len_loss):.4f} '\n",
        "                        f'loss_v: {(self.V_loss.sum()/self.len_loss):.4f} loss_e: {(self.E_loss.sum()/self.len_loss):.4f} loss: {(self.total_loss.sum()/self.len_loss):.4f} '\n",
        "                        f'episode_time: {datetime.now() - episode_time}\\n')\n",
        "            f_out.close()\n",
        "\n",
        "        if total_reward > self.max_test_score or info['flag_get']:\n",
        "            imageio.mimsave('test_episode.mp4', images)\n",
        "            self.max_test_score = total_reward\n",
        "            if is_training:\n",
        "                torch.save(self.model.state_dict(), f\"best_model.pth\")\n",
        "\n",
        "        # if model can complete this game, stop training by set self.is_completed to True\n",
        "        if info['flag_get']:\n",
        "            self.is_completed = True\n",
        "\n",
        "    def save_model(self):\n",
        "        torch.save(self.model.state_dict(), f\"model_{self.current_step}.pth\")\n",
        "\n",
        "    def load_model(self, model_path = None):\n",
        "        if model_path is None:\n",
        "            model_path = f\"model_{self.current_step}.pth\"\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    def select_action(self, states):\n",
        "        # select action when training, we need use Categorical distribution to make action base on probability from model\n",
        "        states = torch.tensor(np.array(states), device = self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits, V = self.model(states)\n",
        "            policy = F.softmax(logits, dim=1)\n",
        "            distribution = torch.distributions.Categorical(policy)\n",
        "            actions = distribution.sample().cpu().numpy().tolist()\n",
        "        return actions, logits, V\n",
        "\n",
        "    def update_loss_statis(self, loss_p, loss_v, loss_e, loss):\n",
        "        # update loss for logging, just save 1000 latest updates.\n",
        "        self.V_loss[self.loss_index] = loss_v\n",
        "        self.P_loss[self.loss_index] = loss_p\n",
        "        self.E_loss[self.loss_index] = loss_e\n",
        "        self.total_loss[self.loss_index] = loss\n",
        "        self.loss_index = (self.loss_index + 1)%1000\n",
        "        self.len_loss = min(self.len_loss+1, 1000)\n",
        "\n",
        "    def learn(self):\n",
        "\n",
        "        # get all data\n",
        "        states, actions, next_states, rewards, dones, logits, values = self.memory.get_data()\n",
        "\n",
        "        # calculate target (td lambda target) and gae advantages\n",
        "        targets = []\n",
        "        with torch.no_grad():\n",
        "            _, next_value = self.model(torch.tensor(np.array(next_states[-1]), device = self.device))\n",
        "        target = next_value\n",
        "        advantage = 0\n",
        "\n",
        "        for state, next_state, reward, done, V in zip(states[::-1], next_states[::-1], rewards[::-1], dones[::-1], values[::-1]):\n",
        "            done = torch.tensor(done, device = self.device, dtype = torch.float).reshape(-1, 1)\n",
        "            reward = torch.tensor(reward, device = self.device).reshape(-1, 1)\n",
        "\n",
        "            target = next_value * self.gamma * (1-done) + reward\n",
        "            advantage = target + self.gamma * advantage * (1-done) * self.gae_lambda\n",
        "            targets.append(advantage)\n",
        "            advantage = advantage - V.detach()\n",
        "            next_value = V.detach()\n",
        "        targets = targets[::-1]\n",
        "\n",
        "        states = torch.tensor(np.array(states), device = self.device)\n",
        "        states = states.reshape(-1, states.shape[2], states.shape[3], states.shape[4])\n",
        "\n",
        "        logits, values = self.model(states)\n",
        "        index = torch.arange(0, len(logits), device = self.device)\n",
        "        actions = torch.flatten(torch.tensor(actions, device = self.device, dtype = torch.int64))\n",
        "\n",
        "        targets = torch.cat(targets, 0).view(-1, 1)\n",
        "        dist = torch.distributions.Categorical(logits = logits)\n",
        "        log_probs = dist.log_prob(actions)\n",
        "        entropy = dist.entropy().mean()\n",
        "\n",
        "        advantages = (targets - values).reshape(-1)\n",
        "        loss_V = F.mse_loss(values, targets)\n",
        "\n",
        "        loss_P = -(log_probs * advantages.detach()).mean()\n",
        "\n",
        "        # backward fisher matrix (This is the only addition over A2C's Adam)\n",
        "        if self.optimizer.steps % self.optimizer.Ts == 0:\n",
        "            self.model.zero_grad()\n",
        "            pg_fisher_loss = -log_probs.mean()  # CELoss\n",
        "            value_noise = torch.randn(values.size()).to(self.device)\n",
        "            sample_values = values + value_noise\n",
        "            vf_fisher_loss = - (values - sample_values.detach()).pow(2).mean()\n",
        "            fisher_loss = pg_fisher_loss + vf_fisher_loss\n",
        "            self.optimizer.acc_stats = True\n",
        "            fisher_loss.backward(retain_graph=True)\n",
        "            self.optimizer.acc_stats = False\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = - entropy * self.entropy_coef + loss_V * self.V_coef + loss_P\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.update_loss_statis(loss_P.item(), loss_V.item(), entropy.item(), loss.item())\n",
        "\n",
        "    def train(self):\n",
        "        episode_reward = [0] * self.num_envs\n",
        "        episode_step = [0] * self.num_envs\n",
        "        max_episode_reward = 0\n",
        "        max_episode_step = 0\n",
        "        episode_time = [datetime.now() for _ in range(self.num_envs)]\n",
        "        total_time = datetime.now()\n",
        "\n",
        "        last_episode_rewards = []\n",
        "\n",
        "        # reset envs\n",
        "        states = self.envs.reset()\n",
        "\n",
        "        while True:\n",
        "            # finish training if agent reach total_step or total_episode based on what type of total_step_or_episode is step or episode\n",
        "            self.current_step += 1\n",
        "\n",
        "            if self.total_step_or_episode == 'step':\n",
        "                if self.current_step >= self.total_step:\n",
        "                    break\n",
        "            else:\n",
        "                if self.current_episode >= self.total_episode:\n",
        "                    break\n",
        "\n",
        "            actions, logit, V = self.select_action(states)\n",
        "\n",
        "            next_states, rewards, dones, truncs, infos = self.envs.step(actions)\n",
        "\n",
        "            # save to memory\n",
        "            self.memory.save(states, actions, rewards, next_states, dones, logit, V)\n",
        "\n",
        "            episode_reward = [x + reward for x, reward in zip(episode_reward, rewards)]\n",
        "            episode_step = [x+1 for x in episode_step]\n",
        "\n",
        "            # logging after each step, if 1 episode is ending, I will log this to logging.txt\n",
        "            for i, done in enumerate(dones):\n",
        "                if done:\n",
        "                    self.current_episode += 1\n",
        "                    max_episode_reward = max(max_episode_reward, episode_reward[i])\n",
        "                    max_episode_step = max(max_episode_step, episode_step[i])\n",
        "                    last_episode_rewards.append(episode_reward[i])\n",
        "                    f_out = open(f\"logging.txt\", \"a\")\n",
        "                    f_out.write(f'episode: {self.current_episode} agent: {i} rewards: {episode_reward[i]:.4f} steps: {episode_step[i]} complete: {infos[i][\"flag_get\"]==True} '\n",
        "                                f'mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean():.4f} max_rewards: {max_episode_reward:.4f} '\n",
        "                                f'max_steps: {max_episode_step} current_step: {self.current_step} loss_p: {(self.P_loss.sum()/self.len_loss):.4f} '\n",
        "                                f'loss_v: {(self.V_loss.sum()/self.len_loss):.4f} loss_e: {(self.E_loss.sum()/self.len_loss):.4f} loss: {(self.total_loss.sum()/self.len_loss):.4f} '\n",
        "                                f'episode_time: {datetime.now() - episode_time[i]} total_time: {datetime.now() - total_time}\\n')\n",
        "                    f_out.close()\n",
        "                    episode_reward[i] = 0\n",
        "                    episode_step[i] = 0\n",
        "                    episode_time[i] = datetime.now()\n",
        "\n",
        "            # training agent every learn_step\n",
        "            if self.current_step % self.learn_step == 0:\n",
        "                self.learn()\n",
        "                self.memory.reset()\n",
        "\n",
        "            if self.current_step % self.save_model_step == 0:\n",
        "                self.save_model()\n",
        "\n",
        "            # eval agent every save_figure_step\n",
        "            if self.current_step % self.save_figure_step == 0 and self.save_figure_step != -1:\n",
        "                self.save_figure(is_training=True)\n",
        "                if self.is_completed:\n",
        "                    f_out = open(f\"logging.txt\", \"a\")\n",
        "                    f_out.write(f' mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean()} max_rewards: {max_episode_reward} '\n",
        "                                f'max_steps: {max_episode_step} current_step: {self.current_step} total_time: {datetime.now() - total_time}\\n')\n",
        "                    f_out.close()\n",
        "                    return\n",
        "\n",
        "            states = list(next_states)\n",
        "\n",
        "        f_out = open(f\"logging.txt\", \"a\")\n",
        "        f_out.write(f' mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean()} max_rewards: {max_episode_reward} '\n",
        "                    f'max_steps: {max_episode_step} current_step: {self.current_step} total_time: {datetime.now() - total_time}\\n')\n",
        "        f_out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718b77f8-186a-4f82-b7e9-330d7ac8098e",
      "metadata": {
        "id": "718b77f8-186a-4f82-b7e9-330d7ac8098e"
      },
      "source": [
        "# Create model\n",
        "Model includes:\n",
        "- 4 convolution layers to encode input image (observation) to feature vector.\n",
        "- 1 linear layers (because KFAC is not implemented for RNN or it is very difficult and does not guarantee that the available KFAC RNN code is bug-free)\n",
        "- two linear layers for policy and value prediction (actor and critic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee1e253-b83d-48d5-aa9c-4463303c3feb",
      "metadata": {
        "id": "dee1e253-b83d-48d5-aa9c-4463303c3feb"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, init_weights):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_dim[0], 32, 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.linear = nn.Linear(1152, 512)\n",
        "        self.critic_linear = nn.Linear(512, 1)\n",
        "        self.actor_linear = nn.Linear(512, output_dim)\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "                nn.init.orthogonal_(module.weight)\n",
        "                # nn.init.kaiming_uniform_(module.weight)\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.LSTMCell):\n",
        "                nn.init.constant_(module.bias_ih, 0)\n",
        "                nn.init.constant_(module.bias_hh, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x/255.))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.linear(x))\n",
        "        return self.actor_linear(x), self.critic_linear(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a7606a-b416-4c17-a3c9-85a6ce07b508",
      "metadata": {
        "id": "c2a7606a-b416-4c17-a3c9-85a6ce07b508"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67741e59",
      "metadata": {
        "id": "67741e59"
      },
      "outputs": [],
      "source": [
        "model = Model(config.state_dim, config.action_dim, config.init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077a241d-419a-4f72-9f2c-7356d7858eae",
      "metadata": {
        "id": "077a241d-419a-4f72-9f2c-7356d7858eae"
      },
      "outputs": [],
      "source": [
        "envs = MultipleEnvironments(config.world, config.stage, config.action_type, config.num_envs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4637f85-679c-4034-befe-14d839485d47",
      "metadata": {
        "id": "e4637f85-679c-4034-befe-14d839485d47"
      },
      "outputs": [],
      "source": [
        "envs.envs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4f5fde-6390-4a81-bb29-223fa70e9539",
      "metadata": {
        "id": "ae4f5fde-6390-4a81-bb29-223fa70e9539"
      },
      "outputs": [],
      "source": [
        "agent = Agent(world = config.world, stage = config.stage, action_type = config.action_type, envs = envs, num_envs = config.num_envs,\n",
        "                  state_dim = config.state_dim, action_dim = config.action_dim, save_dir = config.save_dir,\n",
        "                  save_model_step = config.save_model_step, save_figure_step = config.save_figure_step, learn_step = config.learn_step,\n",
        "                  total_step_or_episode = config.total_step_or_episode, total_step = config.total_step, total_episode = config.total_episode,\n",
        "                  model = model, gamma = config.gamma, learning_rate = config.learning_rate, entropy_coef = config.entropy_coef, \n",
        "                  V_coef = config.V_coef, kfac_momentum = config.kfac_momentum, stat_decay = config.stat_decay, kfac_kl_clip = config.kfac_kl_clip, \n",
        "                  kfac_damping = config.kfac_damping, weight_decay = config.weight_decay, kfac_fast_cnn = config.kfac_fast_cnn, \n",
        "                  kfac_Ts = config.kfac_Ts, kfac_Tf = config.kfac_Tf, gae_lambda = config.gae_lambda,\n",
        "                  eigen_eps = config.eigen_eps, device = \"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sfKd6h_yufaV",
      "metadata": {
        "id": "sfKd6h_yufaV"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f94e0c8-e8c7-4f37-ae4b-ffc9083b6aca",
      "metadata": {
        "id": "5f94e0c8-e8c7-4f37-ae4b-ffc9083b6aca"
      },
      "outputs": [],
      "source": [
        "agent.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc4c92e-b3d8-4431-a604-432aab653ad7",
      "metadata": {
        "id": "2dc4c92e-b3d8-4431-a604-432aab653ad7"
      },
      "outputs": [],
      "source": [
        "agent.current_step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NLjlwqf7qdZK",
      "metadata": {
        "id": "NLjlwqf7qdZK"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4hrG4FCepxby",
      "metadata": {
        "id": "4hrG4FCepxby"
      },
      "outputs": [],
      "source": [
        "agent.load_model(\"best_model.pth\")\n",
        "agent.save_figure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EvVY0vqQqFJB",
      "metadata": {
        "id": "EvVY0vqQqFJB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "689dcf20-f5e3-42b7-81d0-436974a7dbf8",
      "metadata": {
        "id": "689dcf20-f5e3-42b7-81d0-436974a7dbf8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
